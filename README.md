# LLMs_Knowledge
Delve into LLMs for biology

# Attention is all you need
[great explanation for attention](https://zhouyifan.net/2022/11/12/20220925-Transformer/)  

[残差连接和归一化](https://developer.volcengine.com/articles/7389519960881496075)  

[ResNet and Inception](https://zhuanlan.zhihu.com/p/161639679)  
